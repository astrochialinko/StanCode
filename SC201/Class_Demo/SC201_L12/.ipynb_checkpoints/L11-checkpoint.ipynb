{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# IMPORTANT\n",
    "\n",
    "**Please remember to save this notebook `L11.ipynb` and `layers.py` as you work on them!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 請輸入 a5 資料夾之所在位置\n",
    "FOLDERNAME = 'Colab\\ Notebooks/SC201_L11'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# this downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd drive/MyDrive/$FOLDERNAME/sc201/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sc201.classifiers.fc_net import *\n",
    "from sc201.data_utils import get_CIFAR10_data\n",
    "from sc201.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from sc201.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Design of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家在上一份作業裡寫出了一套 fully-connected two-layer neural network，這套網路的架構算是比較簡單，可以透由單一函數來計算 loss 與 gradients。不過要能夠建立更複雜更多層的神經網路，我們必須將網路的架構模組化，將每一層分開定義，再將其拼湊在一起，組合成不同的 network。\n",
    "\n",
    "模組化的原則如下：\n",
    "\n",
    "我們分開定義每一種 layer 類型（如 affine、relu、normalization、dropout、softmax），為每一層定義 `forward` 及 `backward` 函數。`forward` 函數會接收前一層的輸出結果和本層要使用的權重等參數，計算並回傳本層的 output，也同時將 backward pass 計算梯度會需要的資訊儲存在一個 cache 中並將其回傳：\n",
    "\n",
    "```python\n",
    "def layer_forward(x, p):\n",
    "  \"\"\" Receive inputs x and parameters p \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, p, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "而 `backward` 函數會接收 cost function 對於下一層的梯度以及 forward pass 回傳的 `cache` 物件，計算並回傳 cost function 對於本層的梯度。\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, p, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dp = # Derivative of loss with respect to p\n",
    "  \n",
    "  return dx, dp\n",
    "```\n",
    "\n",
    "有了這些 layer，要怎麼組合出完整的 neural network？以作業4的 `TwoLayerNet` 為例，它的架構就會是 affine - relu - affine - softmax。我們會在神經網路初始化時定義它的權重 W1、W2 及偏差 b1、b2。\n",
    "\n",
    "執行 forward pass 的時候，我們將 input X (圖像的畫素) 丟入第一層 affine layer 的 affine_forward 函數，然後將這一層的輸出結果丟入 relu_forward，以此類推，直到最後用 softmax_forward 及 input 的 true label y 計算出網路的預測與正確標籤之間的 cost。\n",
    "\n",
    "`affine_forward(X, W1, b1)` $\\rightarrow$ `relu_forward(...)` $\\rightarrow$ `affine_forward(..., W2, b2)`  $\\rightarrow$ `softmax_forward(..., y)` $\\rightarrow$ $J$\n",
    "\n",
    "這個流程中產生的所有 cache 我們會先儲存起來。執行 backward pass 的時候，我們會將 cost 以及 softmax 在 forward pass 產生的 cache 丟入 `softmax_backward`。這個函數輸出的結果會是 cost function 對於第二層 affine layer 的 output 之梯度。我們再將這個梯度及第二層 affine layer 的 cache 丟入 `affine_backward`，得到 cost function 對於 relu 結果、W2 和 b2的梯度，以此類推，最後計算出 cost function 對於所有參數 W1、b1、W2和 b2 的梯度。這些梯度就可以拿來做 gradient descent 的 update。\n",
    "\n",
    "$J$ $\\rightarrow$ `softmax_backward(..., cache_softmax)` $\\rightarrow$ `affine_backward(..., cache_affine2)` $\\rightarrow$ `relu_backward(..., cache_relu)` (and $dJ/dW2$, $dJ/db2$) $\\rightarrow$ `affine_backward(..., cache_affine1)` $\\rightarrow$ $dJ/dX$ (and $dJ/dW1$, $dJ/db1$)\n",
    "\n",
    "最後要使用模型做預測時，我們可以執行 forward pass，在計算完第二層 `affine_forward` 後提前終止，找出這層輸出的 class scores 中最高的對應類別。\n",
    "\n",
    "要建立更複雜的 neural network 的話，我們只需將適當的 `forward` 及 `backward` 函數插入上方流程即可！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example module with `affine`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請打開 `layers.py` 並詳讀 `affine_forward` 及 `affine_backward` 函數，以了解其運作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: forward\n",
    "請寫出 `layers.py` 檔案中的 `relu_forward` 函數，然後執行下方 code 以做檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: backward\n",
    "請完成同個檔案中的 `relu_backward` 函數，然後執行下方 code 以檢查 gradient 數值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Dropout forward pass\n",
    "Dropout 是神經網路的一種標準化 (regularization) 方式，在 forward pass 的流程中會將前一層輸出結果的其中一些數值隨機重設為零 [1] 。\n",
    "\n",
    "[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)\n",
    "\n",
    "請寫出 `layers.py` 檔案中的 `dropout_forward` 函數，然後執行下方 code 以做檢查。記得 dropout 在 training 和 testing 階段的運作模式不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(500, 500) + 10\n",
    "\n",
    "for p in [0.25, 0.4, 0.7]:\n",
    "  out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n",
    "  out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "  print('Running tests with p = ', p)\n",
    "  print('Mean of input: ', x.mean())\n",
    "  print('Mean of train-time output: ', out.mean())\n",
    "  print('Mean of test-time output: ', out_test.mean())\n",
    "  print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "  print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Please explain why your results in the above cell makes sense.\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 2:\n",
    "What happens if we do not divide the values being passed through dropout by `p` in the dropout layer? Why does that happen?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout backward pass\n",
    "請完成同個檔案中的 `dropout_backward` 函數，然後執行下方 code 以檢查 gradient 數值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 123}\n",
    "out, cache = dropout_forward(x, dropout_param)\n",
    "dx = dropout_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "# Error should be around e-10 or less\n",
    "print('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-connected nets with Dropout\n",
    "我們在 `classifiers/fc_net.py` 檔案裡已經用 modular design 幫大家組合出一個更強大的 `FullyConnectedNet` 神經網路。這個網路的架構是：\n",
    "\n",
    "`{affine - batch/layer norm - relu - dropout} x (L - 1) - affine - softmax`\n",
    "\n",
    "我們可以任意決定網路 layer 的數目 $L$。這裡使用的 relu 和 dropout layers 是大家上面完成的模組。網路的 constructor 用法如下：\n",
    "\n",
    "```python\n",
    "FullyConnectedNet(hidden_dims, input_dim, num_classes, dropout, normalization,\n",
    "                  reg, weight_scale, dtype, seed)\n",
    "```\n",
    "\n",
    "`hidden_dims` 儲存網路所有 hidden (affine) layers 的大小，所以 `len(hidden_dims) = L`。`dropout` 決定 dropout layers 的 `p` 參數，如果 `dropout` 為一，我們就略過 dropout layers。\n",
    "\n",
    "請詳讀並執行以下 code 以了解 `FullyConnectedNet` 的使用方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for dropout in [1, 0.75, 0.5]:\n",
    "  print('Running check with dropout = ', dropout)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            weight_scale=5e-2, dtype=np.float64,\n",
    "                            dropout=dropout, seed=123)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Relative errors should be around e-6 or less; Note that it's fine\n",
    "  # if for dropout=1 you have W2 error be on the order of e-5.\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization experiment\n",
    "\n",
    "現在大家來做個實驗！我們先建立兩套 two-layer network，第一個不使用 dropout，而第二個則隨機保留 ~25% 的神經元。我們接著使用 CIFAR-10 的一部分資料做訓練，並觀測兩種網路的 training 和 validation accuracy。這裡的 training 會用到我們提供的 Solver 套件，Solver 可以指定模型學習的模式（SGD、SGD + momentum、RMSProp 和 Adam）和相關的 hyperparameters，用法請參考以下 code 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train two identical nets, one with dropout and one without\n",
    "np.random.seed(231)\n",
    "num_train = 500\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "dropout_choices = [1, 0.25]\n",
    "for dropout in dropout_choices:\n",
    "  model = FullyConnectedNet([500], dropout=dropout)\n",
    "  print('Dropout: ', dropout)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=25, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-4,\n",
    "                  },\n",
    "                  verbose=True, print_every=100)\n",
    "  solver.train()\n",
    "  solvers[dropout] = solver\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation accuracies of the two models\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for dropout in dropout_choices:\n",
    "  solver = solvers[dropout]\n",
    "  train_accs.append(solver.train_acc_history[-1])\n",
    "  val_accs.append(solver.val_acc_history[-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].train_acc_history, 'o', label='%.2f dropout' % dropout)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].val_acc_history, 'o', label='%.2f dropout' % dropout)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 3:\n",
    "Compare the validation and training accuracies with and without dropout -- what do your results suggest about dropout as a regularization technique?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
